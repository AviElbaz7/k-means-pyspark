{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea933ca9-1532-4406-b540-e6926a62824d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac689458-c1b8-4aba-a47f-feabc3bcb8db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def minMax_Scale(data):\n",
    "    \"\"\"\n",
    "    Function to Min Max Scale the Data using sklearn MinMaxScaler.\n",
    "\n",
    "    :param data: (pyspark.sql.dataframe.DataFrame) The data.\n",
    "    :return: pyspark.sql.dataframe.DataFrame: Normalized Data\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    # Cast to Pandas DataFrame\n",
    "    pandas_df = data.toPandas()\n",
    "    # Scale\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(pandas_df), columns=pandas_df.columns)\n",
    "    # Slice off the last column and convert back to a Spark DataFrame\n",
    "    scaled_data = scaled_data.iloc[:, :-1]\n",
    "    scaled_spark = spark.createDataFrame(scaled_data)\n",
    "    return scaled_spark\n",
    "\n",
    "\n",
    "def Kmeans(data, k, ct=0.0001, iterations=30, initial_centroids=None):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering using MapReduce.\n",
    "\n",
    "    Parameters:\n",
    "        data (pyspark.sql.dataframe.DataFrame): A dataset in pyspark.sql.dataframe.DataFrame format\n",
    "        k: the number of clusters\n",
    "        ct: Convergence threshold (parameter - default is set to 0.0001)\n",
    "        iterations: Number of iteration per experiment (parameter - default is set to 30)\n",
    "        initial_centroids (list): - List of initial centroid locations where each centroid is represented by a tuple of the location\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the centroids calculated by the algorithm so that each centroid is represented by a tuple of its location\n",
    "    \"\"\"\n",
    "    # Min Max Scale\n",
    "    data = minMax_Scale(data)\n",
    "    # Cast to RDD of NumPy arrays\n",
    "    rdd = data.rdd.map(lambda row: np.array(row))\n",
    "\n",
    "    # Initialize centroids if not entered by the user by takeSample(False, k)\n",
    "    if initial_centroids is None:\n",
    "        centroids = rdd.takeSample(False, k)\n",
    "    else:\n",
    "        centroids = initial_centroids\n",
    "\n",
    "    # Preform Iterations\n",
    "    for i in range(iterations):\n",
    "        # Function for the Map step\n",
    "        def Map_Step(point):\n",
    "            \"\"\"\n",
    "            Assign points to the nearest centroid\n",
    "\n",
    "            Parameters:\n",
    "                point: point to assign to new cluster\n",
    "\n",
    "            Returns:\n",
    "                nearest centroid of the point\n",
    "            \"\"\"\n",
    "            distances = [np.linalg.norm(point - centroid) for centroid in centroids]\n",
    "            return np.argmin(distances), point\n",
    "        \n",
    "        # Reduce - Recalculate centroids as the mean of assigned points\n",
    "        def Reduce_Step(point1, point2):\n",
    "            \"\"\"\n",
    "            Combines two points\n",
    "\n",
    "            Parameters:\n",
    "                point1: First point\n",
    "                point2: Second point\n",
    "\n",
    "            Returns:\n",
    "                points combined\n",
    "            \"\"\"\n",
    "            return point1[0] + point2[0], point1[1] + point2[1]\n",
    "        \n",
    "        # Send to all workers\n",
    "        bc_centroids = sc.broadcast(centroids)\n",
    "\n",
    "        # For each data point xi: Find the nearest centroid and Assign the point to that cluster\n",
    "        temp_clustered_points = rdd.map(Map_Step)\n",
    "\n",
    "        # Calculate new centroids\n",
    "        new_centroids = (\n",
    "            temp_clustered_points\n",
    "            .mapValues(lambda point: (point, 1))  # Map step on points\n",
    "            .reduceByKey(Reduce_Step)  # Reduce Step\n",
    "            .mapValues(lambda x: x[0] / x[1])  # Calculate the mean for each cluster\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        # For each cluster j=1,â€¦,k: new centroid = average of all points assigned to cluster c\n",
    "        new_centroids = [centroid[1] for centroid in sorted(new_centroids)]\n",
    "\n",
    "        # Check if the change in centroid positions is less than the specified threshold (ct).\n",
    "        check_converged = True\n",
    "        for new, old in zip(new_centroids, centroids):\n",
    "            if np.linalg.norm(np.array(new) - np.array(old)) >= ct:\n",
    "                check_converged = False\n",
    "                break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "        # Break if the change in centroid positions is less than the specified threshold (ct).\n",
    "        if check_converged:\n",
    "            break\n",
    "    \n",
    "    final_centroids = [tuple(round(num, 5) for num in tup) for tup in centroids]\n",
    "    return final_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca142aa-d60f-4c8b-ba22-f72b9023b0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test passed successfully\nThe test passed successfully\n"
     ]
    }
   ],
   "source": [
    "# \"\"\" ******************* Importing the data ******************* \"\"\"\n",
    "\n",
    "Iris = \"/FileStore/tables/Iris.csv\"\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"True\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "iris = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(Iris)\n",
    "\n",
    "\"\"\" ******************* Example for testing ******************* \"\"\"\n",
    "new_centroids = Kmeans(iris, 2, initial_centroids=[(0.5,0.5,0.5,0.5,0.5),(0.3,0.3,0.3,0.3,0.3)])\n",
    "round_new_centroids=[tuple(round(num, 5) for num in tup) for tup in new_centroids]\n",
    "expected_new_centroids=[(0.16871,0.19553,0.58252,0.08475,0.06618),(0.67067,0.54882,0.36532,0.66478,0.65951)]\n",
    "if (not len(new_centroids)==len(expected_new_centroids)):\n",
    "    print(\"Failed - Number of clusters is different than requested\")\n",
    "if set(round_new_centroids)==set(expected_new_centroids):\n",
    "    print(\"The test passed successfully\")\n",
    "else:\n",
    "    print(\"The test failed\")\n",
    "\n",
    "new_centroids = Kmeans(iris, 3, initial_centroids=[(0.7,0.7,0.7,0.7,0.7),(0.5,0.5,0.5,0.5,0.5),(0.3,0.3,0.3,0.3,0.3)])\n",
    "round_new_centroids=[tuple(round(num, 5) for num in tup) for tup in new_centroids]\n",
    "expected_new_centroids=[(0.84235, 0.65366, 0.41933, 0.77894, 0.81117),\n",
    " (0.51298, 0.44864, 0.31368, 0.55836, 0.51965),\n",
    " (0.16443, 0.19611, 0.59083, 0.07864, 0.06)]\n",
    "if (not len(new_centroids)==len(expected_new_centroids)):\n",
    "    print(\"Failed - Number of clusters is different than requested\")\n",
    "if set(round_new_centroids)==set(expected_new_centroids):\n",
    "    print(\"The test passed successfully\")\n",
    "else:\n",
    "    print(\"The test failed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "k means pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
